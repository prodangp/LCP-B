{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# AdaBoost Algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Gradient Boosting \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prody/anaconda3/envs/unipd/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# XGBoost \n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance, to_graphviz, plot_tree\n",
    "print(\"XGBoost version:\",xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "CASE = 1\n",
    "\n",
    "S = 2\n",
    "N = 2000\n",
    "N_train = 1000\n",
    "N_test  = N - N_train\n",
    "x = S*(2*np.random.rand(N,2)-1)\n",
    "y = np.zeros(N).astype(int)\n",
    "\n",
    "for n in range(N):\n",
    "    if CASE==1:\n",
    "        if x[n,1]<-0.6 and x[n,0]>-0.2: y[n]=1\n",
    "        if x[n,1]>0.4 and x[n,0]<-0.8: y[n]=1\n",
    "        if x[n,1]>1.0 and x[n,0]>0.8: y[n]=1\n",
    "    elif CASE==2:\n",
    "        if x[n,1]<0 and x[n,0]>0.5: y[n]=1\n",
    "        if x[n,1]>0 and np.sqrt((x[n,0]+0.3)**2+x[n,1]**2)<1.5: y[n]=1\n",
    "\n",
    "x_train,y_train = x[:N_train],y[:N_train]\n",
    "x_test,y_test = x[N_train:],y[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorater used to block function printing to the console\n",
    "import os, sys\n",
    "def blockPrinting(func):\n",
    "    def func_wrapper(*args, **kwargs):\n",
    "        # block all printing to the console\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        # call the method in question\n",
    "        value = func(*args, **kwargs)\n",
    "        # enable all printing to the console\n",
    "        sys.stdout = sys.__stdout__\n",
    "        # pass the return value of the method back\n",
    "        return value\n",
    "\n",
    "    return func_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@blockPrinting\n",
    "def classify(clf=GradientBoostingClassifier(),show=False):\n",
    "    # GradientBoostingClassifier():\n",
    "    #   n_estimators = 100 (default)\n",
    "    #   loss function = deviance(default) used in Logistic Regression\n",
    "    # XGBClassifier()\n",
    "    #   n_estimators = 100 (default)\n",
    "    #   max_depth = 3 (default)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_hat = clf.predict(x_test)\n",
    "    err = 100*(1-clf.score(x_test, y_test))\n",
    "    #if CASE<10: print(\"errors: {:.2f}%\".format(err))\n",
    "\n",
    "    if show:    \n",
    "        dx = 0.02\n",
    "        x_seq=np.arange(-S,S+dx,dx)\n",
    "        nx = len(x_seq)\n",
    "        x_plot=np.zeros((nx*nx,2))\n",
    "        q=0\n",
    "        for i in range(nx):\n",
    "            for j in range(nx):\n",
    "                x_plot[q,:] = [x_seq[i],x_seq[j]]\n",
    "                q+=1\n",
    "        y_plot= clf.predict(x_plot)\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.title(str(clf))\n",
    "\n",
    "        scat(x_plot,y_plot,cmap=\"winter\",s=1)\n",
    "        scat(x_train,y_train,s=7)\n",
    "        plt.show()\n",
    "    \n",
    "#     if show:      \n",
    "#         dump_list = clf.get_booster().get_dump()\n",
    "#         num_trees = len(dump_list)\n",
    "#         print(\"num_trees=\",num_trees)\n",
    "        \n",
    "#         fig, AX = plt.subplots(3,1,figsize=(30, 30))\n",
    "#         for i in range(min(3,num_trees)):\n",
    "#             ax=AX[i]\n",
    "#             plot_tree(clf, num_trees=i, ax=ax)\n",
    "#         fig.savefig(\"DATA/tree-classif.png\", dpi=300, pad_inches=0.02)   \n",
    "#         plt.show()\n",
    "\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For the labeling of simple two dimensional data (as the one generated during the lesson), try different\n",
    "parameters (gamma, lambda, n_estimators, ...), aiming to find the simplest yet effective XGBoost\n",
    "model that keeps a good accuracy.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The defaults for XGBClassifier are:\n",
    "#     max_depth=3\n",
    "#     learning_rate=0.1\n",
    "#     n_estimators=100\n",
    "#     silent=True\n",
    "#     objective='binary:logistic'\n",
    "#     booster='gbtree'\n",
    "#     n_jobs=1\n",
    "#     nthread=None\n",
    "#     gamma=0\n",
    "#     min_child_weight=1\n",
    "#     max_delta_step=0\n",
    "#     subsample=1\n",
    "#     colsample_bytree=1\n",
    "#     colsample_bylevel=1\n",
    "#     reg_alpha=0\n",
    "#     reg_lambda=1\n",
    "#     scale_pos_weight=1\n",
    "#     base_score=0.5\n",
    "#     random_state=0\n",
    "#     seed=None\n",
    "#     missing=None\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! Here we plot the results:\n",
      "learning rate 0.0001 err = 0.2 %\n",
      "learning rate 0.001 err = 0.2 %\n",
      "learning rate 0.01 err = 0.2 %\n",
      "learning rate 0.1 err = 0.3 %\n",
      "learning rate 1 err = 0.5 %\n",
      "learning rate 10 err = 0.2 %\n",
      "n_estimators 1 err = 0.2 %\n",
      "n_estimators 10 err = 0.2 %\n",
      "n_estimators 50 err = 0.2 %\n",
      "n_estimators 100 err = 0.2 %\n",
      "n_estimators 200 err = 0.2 %\n",
      "n_estimators 1000 err = 0.3 %\n",
      "gamma 0.0 err = 0.2 %\n",
      "gamma 1.0 err = 0.2 %\n",
      "gamma 2.0 err = 0.2 %\n",
      "gamma 4.0 err = 0.2 %\n",
      "gamma 10.0 err = 0.2 %\n",
      "gamma 20.0 err = 0.3 %\n",
      "gamma 40.0 err = 0.3 %\n",
      "gamma 100.0 err = 1.2 %\n",
      "gamma 200.0 err = 29.8 %\n",
      "lambda 100.0 err = 1.4 %\n",
      "lambda 30 err = 0.7 %\n",
      "lambda 20.0 err = 0.4 %\n",
      "lambda 10.0 err = 0.3 %\n",
      "lambda 5.0 err = 0.2 %\n",
      "lambda 1.0 err = 0.2 %\n",
      "lambda 0.5 err = 0.2 %\n",
      "lambda 0.1 err = 0.2 %\n"
     ]
    }
   ],
   "source": [
    "err_ = []\n",
    "lr_ = [1e-4, 0.001, 0.01, 0.1, 1, 10]\n",
    "print('!!! Here we plot the results:')\n",
    "for lr in lr_:\n",
    "    \n",
    "    err_.append(classify(XGBClassifier(seed=1, learning_rate=lr),show=False))\n",
    "    \n",
    "for idx, lr in enumerate(lr_):\n",
    "    print(f'learning rate {lr} err = {round(err_[idx], 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best learning rate is for 0.2%, so either 0.0001, 0.001, 0.01 or 10. We are using 0.01 for next experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_ = []\n",
    "par_ = [1, 10, 50, 100, 200, 1000]\n",
    "#par_ = [1, 2, 5, 10]\n",
    "for par in par_:\n",
    "    err_.append(classify(XGBClassifier(seed=1, n_estimators=par, learning_rate=0.01),show=False))\n",
    "    \n",
    "for idx, par in enumerate(par_):\n",
    "    print(f'n_estimators {par} err = {round(err_[idx], 4)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error does not change when varying n_estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_ = []\n",
    "par_ = [0.,1.,2.,4.,10.,20.,40.,100.,200.]\n",
    "for par in par_:\n",
    "    err_.append(classify(XGBClassifier(seed=1, learning_rate=0.01, gamma=par),show=False))\n",
    "    \n",
    "for idx, par in enumerate(par_):\n",
    "    print(f'gamma {par} err = {round(err_[idx], 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma 0.0 = 0.2 %\n",
    "gamma 1.0 = 0.2 %\n",
    "gamma 2.0 = 0.2 %\n",
    "gamma 4.0 = 0.2 %\n",
    "gamma 10.0 = 0.2 %\n",
    "gamma 20.0 = 0.3 %\n",
    "gamma 40.0 = 0.3 %\n",
    "gamma 100.0 = 1.2 %\n",
    "gamma 200.0 = 29.8 %\n",
    "\n",
    "Small gamma leads to best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_ = []\n",
    "par_ = [100.,30,20.,10.,5.,1.,0.5,0.1]\n",
    "for par in par_:\n",
    "    err_.append(classify(XGBClassifier(seed=1, learning_rate=0.01, gamma=1, reg_lambda=par),show=False))\n",
    "    \n",
    "for idx, par in enumerate(par_):\n",
    "    print(f'lambda {par} err = {round(err_[idx], 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda 100.0 = 1.4 %\n",
    "lambda 30 = 0.7 %\n",
    "lambda 20.0 = 0.4 %\n",
    "lambda 10.0 = 0.3 %\n",
    "lambda 5.0 = 0.2 %\n",
    "lambda 1.0 = 0.2 %\n",
    "lambda 0.5 = 0.2 %\n",
    "lambda 0.1 = 0.2 %\n",
    "Also, small reg_lambda leads to best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unipd",
   "language": "python",
   "name": "unipd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
